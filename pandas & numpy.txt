
#Lambda Functions

-is_substring = lambda my_string: my_string in "This is the master string"
-check_if_A_grade = lambda grade: 'Got an A!' if grade >= 90 else 'Did not get an A...'
-long_string = lambda str: len(str) > 12
-ends_in_a = lambda str: str[-1] == 'a'



##Random
	import random
	add_random = lambda num: num + random.randint(1,10)



#Object creation
s = pd.Series([1, 3, 5, np.nan, 6, 8])



#dataframe list
df2 = pd.DataFrame([
    ['John Smith', '123 Main St.', 34],
    ['Jane Doe', '456 Maple Ave.', 28],
    ['Joe Schmo', '789 Broadway', 51]
    ],
    columns=['name', 'address', 'age'])


#CSVs
pd.read_csv('my.csv')

df.to_csv('new-csv-file.csv')										**saving to a csv**
	
df.head(10)												**show the first 10 rows**

df.info()												**gives some statistics for each column**

new_df = orders[['last_name', 'email']]									**select two columns from orders**


##Selecting rows
df.iloc[0, 4, 6]											**select row number 0 in df **
orders.iloc[-3:]
orders.iloc[:4]


df[df.MyColumnName == desired_column_value]								**like: df[df.age < 30] or df[df.name != 'Clara Oswald']**

df[(df.month == 'March') & (df.month == 'April')]							**use & and | to select multiple row**

df[df.month.isin(['January','February','March'])]


##Setting indices
df.reset_index(drop=True)										**our selected non-consecutive DataFrame with a new set of ordered  indices**
df.reset_index(inplace=True, drop=True)									**modify our existing DataFrame**

!inplace=True means overwrite excisting column

#Adding a Column
df['Quantity'] = [100, 150, 50, 35]									**it should have same amount of rows**
df['Margin'] = df.Price - df['Cost to Manufacture']							**we can use logic on other columns**
df['Name'] = df.Name.apply(str.upper)


##lambda for strings
stringlambda = lambda x: x.lower()
mylambda = lambda x: x[0]+x[-1]										**gets the first and ladt letter**
myfunction = lambda x: x  * 1.50 if x > 40 else x

df['Email Provider'] = df.Email.apply(									**split the emails**
    lambda x: x.split('@')[-1]	)

####if example
df['Price with Tax'] = df.apply(lambda row:
     row['Price'] * 1.075
     if row['Is taxed?'] == 'Yes'
     else row['Price'],
     axis=1)

!axis=1 is important because it lets the input of the lambda function be an entire row


#Renaming Columns
df.columns = ['First Name', 'Age']									**rename all existing columns**

df.rename(columns={
    'name': 'First Name',
    'age': 'Age'},
    inplace=True)



---
#Calculating Column Statistics
print(inventory.color.unique())

Command		Description
mean		Average of all values in column
std		Standard deviation
median		Median
max		Maximum value in column
min		Minimum value in column
count		Number of values in column
nunique		Number of unique values in column
unique		List of unique values in column

.isnull()

#Calculating Aggregate Functions
df.groupby('column1').column2.measurement()								**column2 is the column that we want to perform a measurement on, measurement is the measurement function we want to apply**


##rename
teas_counts = teas_counts.rename(columns={"id": "counts"})

###np.percentile											**np.percentile can calculate any percentile over an array of values**
high_earners = df.groupby('category').wage
    .apply(lambda x: np.percentile(x, 75))
    .reset_index()


##group by multiple columns
shoe_counts = orders.groupby(['shoe_type', 'shoe_color']).id.count().reset_index()

###pivot tables
# First use the groupby statement:
unpivoted = df.groupby(['Location', 'Day of Week'])['Total Sales'].mean().reset_index()
# Now pivot the table
pivoted = unpivoted.pivot(
    columns='Day of Week',
    index='Location',
    values='Total Sales')



---
merg tables

#Inner Merge (A merge where only matching rows are included.)

new_df = pd.merge(orders, customers)

big_df = orders.merge(customers)\									**merge orders to customers, and then the resulting DataFrame to products**
    .merge(products)

pd.merge(orders,											**rename id for tables have same column**
    customers.rename(columns={'id': 'customer_id'}))


##left_on and right_on											**merge the order and customer table on columns where orders.customers_id is the same as customers.id **
pd.merge(
    orders,
    customers,
    left_on='customer_id',
    right_on='id',
    suffixes=['_order', '_customer']									**name the columns id_orders and id_customers**
)


#Outer Merge (A merge where all rows are included, whether they match or not.)

pd.merge(company_a, company_b, how='outer')

pd.merge(company_a, company_b, how='left')								**have every merge column on first table that are not nan**

pd.merge(company_a, company_b, how='right')								**have every merge column on second table that are not nan but left one have nan and compares to right**

pd.concat([df1, df2, df3, ...])										**Concatenate DataFrames, only works if all of the columns are the same in all of the DataFrames**



---
#datatypes
mydf.dtypes


#.info() 
mydf.info() 


##.astype() function can be used to convert between a numerical data types, including:
cereal['name'] = cereal['name'].astype("string")
int32 int64 / float32 float64 / object / string / bool


### .fillna() method replaces the NULL values with a specified value. 
movies['cast_count'].fillna(0, inplace = True)


##.Categorical()
cereal['shelf'] = pd.Categorical(cereal['shelf'], ['bottom', 'mid', 'top'], ordered=True)		**takes shlef column and categrizr from order given**

##.get_dummies()											**this will seprate and creat new columns from my_column**
my_df = pd.get_dummies(my_df, columns=['my_column'])	

##.cat.codes												**assigns a numerical value to the (ordinal) categorical variables.**
auto['engine_codes'] = auto['engine_size'].cat.codes						


##.loc													**return of specified rows and/or columns from that DataFrame**
df.loc[:2]
df.loc[:,'col 3']
df.loc[[0,3],'col 2':]
df.loc[df['shield'] > 6]


##.value_counts()	
returns object containing counts of unique values like df.value_counts()


##.describe()												**calculate summary statistics **
df.describe()
df.describe(include='all')

#replace none value 
df[['col 1', 'col 2']] = df[['col 1', 'col 2']].replace(0, np.NaN)					**this will replace 0 on col 1 and 2 woth nan**



---
#Central Tendency for Quantitative Data

## Mean
rentals.rent.mean()

## Median
rentals.rent.median()

## Mode
rentals.rent.mode()

## Trimmed mean
from scipy.stats import trim_mean
trim_mean(rentals.rent, proportiontocut=0.1)



#Spread for Quantitative Data

## Range
rentals.rent.max() - rentals.rent.min()

## Interquartile range
rentals.rent.quantile(0.75) - rentals.rent.quantile(0.25)

from scipy.stats import iqr
iqr(rentals.rent)  # alternative way

## Variance
rentals.rent.var()

## Standard deviation
rentals.rent.std()

## Mean absolute deviation
rentals.rent.mad()



# Proportions of rental listings in each borough
##-calculate the proportion for each category
rentals.borough.value_counts() / len(rentals.borough)


###select a row from specific of another row
mydf.col1[mydf.col2 == 'U']



----
#Covariance
np.cov(mydf.price, mydf.sqfeet)


#Correlation
from scipy.stats import pearsonr
corr_price_sqfeet, p = pearsonr(housing.price, housing.sqfeet)


## marginal proportion
freq = pd.crosstab(mydf.special, mydf.authority)
prop = freq/len(mydf)
leader_marginals = prop.sum(axis=0)
print(leader_marginals)
influence_marginals =  prop.sum(axis=1)
print(influence_marginals)



##frequencies  chi2_contingency()
from scipy.stats import chi2_contingency
chi2, pval, dof, expected = chi2_contingency(freq)
print(np.round(expected))



##.intersection()												
inter = a.intersection(b)


----
#probability


##random
np.random.choice(die_6, size = 100, replace = True)								**returns a set that contains the similarity between two or more sets**


##binom.pmf()
import scipy.stats as stats
#### st.binom.pmf(x, n, p)   											**x: the value of interest / n: the number of trials / p: the probability of success**
print(stats.binom.pmf(6, 10, 0.5))										**lip a fair coin 10 times and count the number of heads**


### calculating P(2-4 heads) = P(2 heads) + P(3 heads) + P(4 heads) for flipping a coin 10 times
print(stats.binom.pmf(2, n=10, p=.5) + stats.binom.pmf(3, n=10, p=.5) + stats.binom.pmf(4, n=10, p=.5))


### less than or equal to 8
1 - (stats.binom.pmf(9, n=10, p=.5) + stats.binom.pmf(10, n=10, p=.5))


##binom.cdf()													**x: the value of interest, looking for the probability of this value or less / n: the sample size / p: the probability of success**
###observing between 4 and 8 heads from 10 fair coin flips
 print(stats.binom.cdf(8, 10, 0.5) - stats.binom.cdf(3, 10, 0.5))

###observing more than 6 heads from 10 fair coin flips
print(1 - stats.binom.cdf(6, 10, 0.5))


##norm.cdf() for discrete or continuous variables
#### stats.norm.cdf(x, loc, scale)
print(stats.norm.cdf(158, 167.64, 8))										**x: the value of interest / loc: the mean of the probability distribution / scale: the standard deviation of the probability distribution**


##poisson.pmf()   probability of observing a specific number given the parameter of a distribution.
#### expected value = 10, probability of observing 6
stats.poisson.pmf(6, 10)											


##poisson.cdf()   probability of observing a specific number or less given the expected value of a distribution.
#### expected value = 10, probability of observing 6 or less
stats.poisson.cdf(6, 10)


##poisson.rvs() 	 generate random values
#### stats.poisson.rvs(lambda, size = num_values)
rvs = stats.poisson.rvs(10, size = 1000)


##percentile of the Poisson
stats.poisson.ppf(percentile, lambda)



##numpy.var() 	variance — or spread —
rand_vars = stats.poisson.rvs(4, size = 1000)
print(np.var(rand_vars))
print(min(rand_vars), max(rand_vars))



##Sampling Distributions
mean of samples in csv

///
population = pd.read_csv("cod_population.csv")
# Save transaction times to a separate numpy array
population = population['Cod_Weight']

sample_size = 50
sample_means = []

for i in range(500):
  samp = np.random.choice(population, sample_size, replace = False)
  # calculate mean here
  this_sample_mean = np.mean(samp)
  # append here
  sample_means.append(this_sample_mean)

///



---
#One-Sample T-Test
tstat, pval = ttest_1samp(sample_distribution, expected_mean)
tstat, pval = ttest_1samp(prices, 1000)


yon = np.random.choice(['y', 'n'], size=500, p=[0.1, 0.9])
np.sum(my_df.column == 'yes-text')
np.sum(yon == 'n)

np.percentile(my_df, [2.5,97.5])


#One-Sided P-Value
null_outcomes = np.array(null_outcomes)
p_value = np.sum(null_outcomes <= 41)/len(null_outcomes)


#Two-Sided P-Value
null_outcomes = np.array(null_outcomes)
p_value = np.sum((null_outcomes <= 41) | (null_outcomes >= 59))/len(null_outcomes) 



#binom_test() 
!requires three inputs, the number of observed successes, the number of total trials, and an expected probability of success.
eg., alternative = 'less' will run a one-sided lower tail test

from scipy import binom_test
p_value = binom_test(2, n=10, p=0.5)
###p_value_1sided here:
p_value_1sided = binom_test(41, n=500, p=0.1, alternative = 'less')



----
#LINEar regression
import matplotlib.pyplot as plt
ex ==>	plt.scatter(students.hours_studied, students.score)


#Fitting a Linear Regression
	import statsmodels.api as sm

	students = pd.read_csv('test_data.csv')

## Create the model here (predict score (it is the outcome variable) using hours_studied as a predictor):
	model = sm.OLS.from_formula('score ~ hours_studied', data = students)
## Fit the model here:
	results = model.fit()
## Print the coefficients (parmas) here. (or print(results.summary()) ):
	print(results.params)
## predict by results
	print(results.params[1] * 160 + results.params[0])
## calculate customize value 
	newdata = {"hours_studied":[5]}
	print(results.predict(newdata))

## Calculate `fitted_values` here:
	fitted_values = results.predict(students)
## Calculate `residuals` here:
	residuals = students.score - fitted_values
	print(residuals.head())


###Normality assumption
plt.hist(residuals)
###Homoscedasticity assumption
plt.scatter(fitted_values, residuals)


##**Categorical Predictors**
## Calculate group means
	print(students.groupby('breakfast').mean().score)
## Create the scatter plot here:
	plt.scatter(students.breakfast, students.score)
## Add the additional line here:
	plt.plot([0,1],[61.66, 73.72])
	plt.show()


## the x matrix
import patsy
rentals = pd.read_csv('rentals.csv')
y, X = patsy.dmatrices('rent ~ borough', rentals)
print(X[0:5])								

##coefficient
print(rentals.groupby('borough').mean())


---
#data cleaning

###mydf.shape			==> .shape method in pandas identifies the number of rows and columns in our dataset as (rows, columns)

###drop_duplicates()		==> .drop_duplicates() function removes duplicate rows
restaurants = restaurants.drop_duplicates()

### map()			==> map() applies the str.lower() function to each of the columns in our dataset to convert the column names to all lowercase 
restaurants.columns = map(str.lower, restaurants.columns)

## rename()			==> the key refers to the original column name and the value refers to the new column name {'oldname1': 'newname1', 'oldname2': 'newname2'}
restaurants = restaurants.rename({'dba': 'name', 'cuisine description': 'cuisine'}, axis=1)

##isna()			==> counts the number of missing values in each column 
restaurants.isna().sum() 

## where()			==> .where() function replaces latitude values less than 40 with NaN values
restaurants['latitude'] = restaurants['latitude'].where(restaurants['latitude'] < 40, np.nan) 

###pd.crosstab()		==> computes the frequency of two or more variables
!!!

###str.lstrip()			==> remove the prefixes
restaurants['url'] = restaurants['url'].str.lstrip('https://') 


###melt()			==> turn the current values in the column headers into row values
!!!


